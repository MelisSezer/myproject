{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri çekme aralığı: 2018-07-12 - 2025-07-10\n",
      "FI bbox 21.164067,59.948690,31.359379,69.915364 içindeki sensörler çekiliyor...\n",
      "Sayfa 1 çekildi, 89 konum eklendi.\n",
      "\n",
      "Toplam 198 sensör için GÜNLÜK veri çekiliyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sensör Verileri Çekiliyor:   4%|▎         | 7/198 [00:27<12:04,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensör 28188 için hata (Sayfa 4): The read operation timed out. Bekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sensör Verileri Çekiliyor: 100%|██████████| 198/198 [16:45<00:00,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "İşlenmemiş ham veri başarıyla 'openaq_raw_data_fi.csv' dosyasına kaydedildi.\n",
      "Toplam 268446 satır ham veri çekildi.\n"
     ]
    }
   ],
   "source": [
    "# BLOK1: APİDEN VERİ ÇEKME \n",
    "import pandas as pd\n",
    "from openaq import OpenAQ\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "API_KEY = \"beb87bdeb247d901be44e59bd044aa34556b5c4752592b5a2cd9da243a25a466\"\n",
    "BBOX_FI = \"21.164067,59.948690,31.359379,69.915364\" # Finlandiya koordinatları\n",
    "COUNTRY_CODE = 'FI'\n",
    "RAW_DATA_FILE = 'openaq_raw_data_fi.csv' # Ham verinin kaydedileceği dosya\n",
    "\n",
    "client = OpenAQ(api_key=API_KEY)\n",
    "\n",
    "def get_all_sensor_ids_in_bbox(bbox, client, country_code):\n",
    "    all_sensor_ids, sensor_country_map = [], {}\n",
    "    page = 1\n",
    "    print(f\"{country_code} bbox {bbox} içindeki sensörler çekiliyor...\")\n",
    "    while True:\n",
    "        try:\n",
    "            resp = client.locations.list(bbox=bbox, limit=1000, page=page)\n",
    "            if not resp.results: break\n",
    "            for loc in resp.results:\n",
    "                if hasattr(loc, 'sensors') and loc.sensors:\n",
    "                    for sensor in loc.sensors:\n",
    "                        sensor_id = getattr(sensor, 'id', None)\n",
    "                        if sensor_id:\n",
    "                            all_sensor_ids.append(sensor_id)\n",
    "                            sensor_country_map[sensor_id] = getattr(loc.country, 'code', country_code)\n",
    "            print(f\"Sayfa {page} çekildi, {len(resp.results)} konum eklendi\")\n",
    "            page += 1; time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print(f\"Hata (Sayfa {page}): {e}. 5 saniye bekleniyor.\"); time.sleep(5)\n",
    "            continue\n",
    "    return list(set(all_sensor_ids)), sensor_country_map\n",
    "\n",
    "def get_daily_by_sid(sids, start, end, sensor_map, sleep_time=0.3):\n",
    "    all_data = []\n",
    "    print(f\"\\nToplam {len(sids)} sensör için GÜNLÜK veri çekiliyor...\")\n",
    "    for sid in tqdm(sids, desc=\"Sensör Verileri Çekiliyor\"):\n",
    "        page = 1\n",
    "        while True:\n",
    "            try:\n",
    "                resp = client.measurements.list(sensors_id=sid, data=\"days\", datetime_from=start, datetime_to=end, limit=1000, page=page)\n",
    "                if not resp.results: break\n",
    "                for res in resp.results:\n",
    "                    all_data.append({\n",
    "                        \"from_date\": res.period.datetime_from.utc,\n",
    "                        \"name\": res.parameter.name,\n",
    "                        \"value\": res.value,\n",
    "                        \"unit\": res.parameter.units,\n",
    "                        \"country\": sensor_map.get(sid, 'Unknown')\n",
    "                    })\n",
    "                page += 1; time.sleep(sleep_time)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nSensör {sid} için hata (Sayfa {page}): {e}. Bekleniyor...\")\n",
    "                try:\n",
    "                    wait_time = int(str(e).split('resets in ')[1].split(' ')[0]) + 1\n",
    "                    print(f\"{wait_time} saniye bekleniyor...\"); time.sleep(wait_time)\n",
    "                except:\n",
    "                    time.sleep(10)\n",
    "                continue\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "bitis = datetime.now()\n",
    "baslangic = bitis - timedelta(days=365 * 7)\n",
    "baslangic_str = baslangic.strftime(\"%Y-%m-%d\")\n",
    "bitis_str = bitis.strftime(\"%Y-%m-%d\")\n",
    "print(f\"Veri çekme aralığı: {baslangic_str} - {bitis_str}\")\n",
    "\n",
    "sensor_ids, sensor_country_map = get_all_sensor_ids_in_bbox(BBOX_FI, client, COUNTRY_CODE)\n",
    "if sensor_ids:\n",
    "    raw_df = get_daily_by_sid(sensor_ids, baslangic_str, bitis_str, sensor_country_map)\n",
    "    if not raw_df.empty:\n",
    "        raw_df.to_csv(RAW_DATA_FILE, index=False)\n",
    "        print(f\"\\nİşlenmemiş ham veri başarıyla '{RAW_DATA_FILE}' dosyasına kaydedildi\")\n",
    "        print(f\"Toplam {len(raw_df)} satır ham veri çekildi\")\n",
    "    else:\n",
    "        print(\"API'den veri çekilemedi\")\n",
    "else:\n",
    "    print(\"Belirtilen alanda sensör bulunamadı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'openaq_raw_data_fi.csv' dosyasından ham veri okunuyor...\n",
      "\n",
      "Birim dönüşümü başlıyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Birimler Dönüştürülüyor: 100%|██████████| 268446/268446 [00:07<00:00, 37272.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Veri işleme ve pivotlama başlıyor...\n",
      "\n",
      "İşlenmiş (logaritmasız) veri 'fi_air_quality_processed.csv' dosyasına kaydedildi.\n",
      "İşlenmiş verinin ilk 5 satırı:\n",
      "                  from_date        no2         o3  pm1       pm10      pm25  \\\n",
      "0 2016-12-09 22:00:00+00:00  13.479308  65.770000  NaN  15.753214  4.704615   \n",
      "1 2016-12-10 22:00:00+00:00  12.045000  65.750000  NaN  10.063571  3.865385   \n",
      "2 2016-12-11 22:00:00+00:00  19.222154  63.310000  NaN   9.663571  4.333077   \n",
      "3 2016-12-12 22:00:00+00:00  23.652923  59.690000  NaN   6.394074  3.754615   \n",
      "4 2016-12-13 22:00:00+00:00  17.254077  56.054545  NaN   9.161429  5.101538   \n",
      "\n",
      "   relativehumidity       so2  temperature  um003  \n",
      "0               NaN  0.799433          NaN    NaN  \n",
      "1               NaN  0.887144          NaN    NaN  \n",
      "2               NaN  0.841889          NaN    NaN  \n",
      "3               NaN  0.764556          NaN    NaN  \n",
      "4               NaN  0.854444          NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# BLOK2: VERİ ÖN İŞLEME \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DATA_FILE = 'openaq_raw_data_fi.csv'\n",
    "PROCESSED_FILE_PATH = 'fi_air_quality_processed.csv'\n",
    "COUNTRY_CODE = 'FI'\n",
    "\n",
    "def convert_and_process_data(file_path, country_code):\n",
    "    print(f\"'{file_path}' dosyasından ham veri okunuyor...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Birim dönüştürme (µg/m³)\n",
    "    print(\"\\nBirim dönüşümü başlıyor...\")\n",
    "    MOLAR_MASSES = {'co': 28.01, 'no2': 46.01, 'o3': 48.00, 'so2': 64.07}\n",
    "    MOLAR_VOLUME = 24.45\n",
    "    converted_degers = []\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Birimler Dönüştürülüyor\"):\n",
    "        param, unit, deger = str(row['name']).lower(), str(row['unit']).lower(), row['deger']\n",
    "        yeni_degeri = None\n",
    "        if pd.notna(deger) and deger >= 0:\n",
    "            if unit == 'µg/m³': yeni_degeri = deger\n",
    "            else:\n",
    "                molar_mass = MOLAR_MASSES.get(param)\n",
    "                if molar_mass is not None:\n",
    "                    if unit == 'ppm': yeni_degeri = (deger * molar_mass * 1000) / MOLAR_VOLUME\n",
    "                    elif unit == 'ppb': yeni_degeri = (deger * molar_mass) / MOLAR_VOLUME\n",
    "                    else: yeni_degeri = deger\n",
    "                else: yeni_degeri = deger\n",
    "        converted_degers.append(yeni_degeri)\n",
    "    df['deger_converted'] = converted_degers\n",
    "    \n",
    "    # Pivotlama\n",
    "    print(\"\\nVeri işleme ve pivotlama başlıyor...\")\n",
    "    df['from_date'] = pd.to_datetime(df['from_date'], errors='coerce')\n",
    "    df.dropna(subset=['from_date', 'deger_converted'], inplace=True)\n",
    "    df_filtered = df[df['country'] == country_code].copy()\n",
    "    \n",
    "    df_pivot = df_filtered.pivot_table(index='from_date', columns='name', degers='deger_converted', aggfunc='mean').reset_index()\n",
    "    df_pivot.columns.name = None\n",
    "    df_pivot.sort_degers(by='from_date', ascending=True, inplace=True)\n",
    "    \n",
    "    return df_pivot\n",
    "\n",
    "try:\n",
    "    processed_df = convert_and_process_data(RAW_DATA_FILE, COUNTRY_CODE)\n",
    "    processed_df.to_csv(PROCESSED_FILE_PATH, index=False)\n",
    "    print(f\"\\nİşlenmiş (logaritmasız) veri '{PROCESSED_FILE_PATH}' dosyasına kaydedildi\")\n",
    "    print(\"İşlenmiş verinin ilk 5 satırı:\")\n",
    "    print(processed_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Hata: '{RAW_DATA_FILE}' bulunamadı. Lütfen önce 1. hücreyi çalıştırıp veriyi indirin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58d933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'fi_air_quality_processed.csv' dosyasından veri okunuyor...\n",
      "Veri başarıyla yüklendi. Boyut: (2885, 9)\n",
      "\n",
      "Eksik veri yönetimi başlıyor...\n",
      "Eksik veri yönetimi tamamlandı.\n",
      "\n",
      "Aşırı aykırı değerler %99.9 persentil ile kırpılıyor...\n",
      "  'no2' sütunu, üst sınır '37.94' ile kırpıldı.\n",
      "  'o3' sütunu, üst sınır '95.70' ile kırpıldı.\n",
      "  'pm1' sütunu, üst sınır '17.04' ile kırpıldı.\n",
      "  'pm10' sütunu, üst sınır '52.72' ile kırpıldı.\n",
      "  'pm25' sütunu, üst sınır '20.67' ile kırpıldı.\n",
      "  'relativehumidity' sütunu, üst sınır '69.20' ile kırpıldı.\n",
      "  'so2' sütunu, üst sınır '9.68' ile kırpıldı.\n",
      "  'temperature' sütunu, üst sınır '19.44' ile kırpıldı.\n",
      "  'um003' sütunu, üst sınır '2998.12' ile kırpıldı.\n",
      "\n",
      "Logaritmik dönüşüm uygulanıyor...\n",
      "\n",
      "Öznitelik mühendisliği başlıyor...\n",
      "Öznitelik mühendisliği tamamlandı.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melis\\AppData\\Local\\Temp\\ipykernel_32576\\835750726.py:28: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True); df.fillna(method='bfill', inplace=True)\n",
      "C:\\Users\\melis\\AppData\\Local\\Temp\\ipykernel_32576\\835750726.py:52: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temizlenmiş ve öznitelik mühendisliği yapılmış veri 'fi_air_quality_engineered.csv' dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "# BLOK3: FEATURE ENGİNEERİNG\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PROCESSED_FILE_PATH = 'fi_air_quality_processed.csv'\n",
    "ENGINEERED_FILE_PATH = 'fi_air_quality_engineered.csv'\n",
    "\n",
    "print(f\"'{PROCESSED_FILE_PATH}' dosyasından veri okunuyor...\")\n",
    "try:\n",
    "    df = pd.read_csv(PROCESSED_FILE_PATH, parse_dates=['from_date'])\n",
    "    df.set_index('from_date', inplace=True)\n",
    "    print(f\"Veri başarıyla yüklendi. Boyut: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Hata: '{PROCESSED_FILE_PATH}' bulunamadı. Lütfen 2. hücreyi çalıştırdığınızdan emin olu.\")\n",
    "    exit()\n",
    "\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Eksik veri interpolasyonu\n",
    "print(\"\\nEksik veri yönetimi başlıyor...\")\n",
    "pollutant_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n",
    "# Ülke sütununu çıkar\n",
    "if 'country' in pollutant_cols: pollutant_cols.remove('country')\n",
    "\n",
    "df[pollutant_cols] = df[pollutant_cols].interpolate(method='time', limit_direction='both')\n",
    "df.fillna(method='ffill', inplace=True); df.fillna(method='bfill', inplace=True)\n",
    "print(\"Eksik veri yönetimi tamamlandı.\")\n",
    "\n",
    "# Aşırı değerleri kırpma\n",
    "print(\"\\nAşırı aykırı değerler %99.9 persentil ile kırpılıyor...\")\n",
    "for col in pollutant_cols:\n",
    "    ust_limit = df[col].quantile(0.999)\n",
    "    df[col] = df[col].clip(ust=ust_limit)\n",
    "    print(f\" '{col}' sütunu, üst sınır '{ust_limit:.2f}' ile kırpıldı.\")\n",
    "\n",
    "# Logaritmik dönüşüm\n",
    "print(\"\\nLogaritmik dönüşüm uygulanıyor...\")\n",
    "for col in pollutant_cols:\n",
    "    df[col] = np.log1p(df[col])\n",
    "\n",
    "print(\"\\nÖznitelik mühendisliği başlıyor...\")\n",
    "df['year'] = df.index.year; df['month'] = df.index.month; df['day_of_week'] = df.index.dayofweek\n",
    "for pollutant in pollutant_cols:\n",
    "    for lag in [1, 3, 7]: df[f'{pollutant}_lag_{lag}d'] = df[pollutant].shift(lag)\n",
    "    for window in [3, 7]: df[f'{pollutant}_roll_mean_{window}d'] = df[pollutant].rolling(window, closed='left').mean()\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "print(\"Öznitelik mühendisliği tamamlandı\")\n",
    "\n",
    "df.to_csv(ENGINEERED_FILE_PATH, index=True)\n",
    "print(f\"\\nTemizlenmiş ve öznitelik mühendisliği yapılmış veri '{ENGINEERED_FILE_PATH}' dosyasına kaydedildi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4659cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'fi_air_quality_engineered.csv' yüklendi. Boyut: (2885, 57)\n",
      "\n",
      ">>> FI: XGBOOST - NO2 modeli eğitiliyor...\n",
      "-> NO2 için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: XGBOOST - O3 modeli eğitiliyor...\n",
      "-> O3 için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: XGBOOST - PM10 modeli eğitiliyor...\n",
      "-> PM10 için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: XGBOOST - PM25 modeli eğitiliyor...\n",
      "-> PM25 için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: XGBOOST - SO2 modeli eğitiliyor...\n",
      "-> SO2 için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: RANDOMFOREST - NO2 modeli eğitiliyor...\n",
      "-> NO2 için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: RANDOMFOREST - O3 modeli eğitiliyor...\n",
      "-> O3 için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: RANDOMFOREST - PM10 modeli eğitiliyor...\n",
      "-> PM10 için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: RANDOMFOREST - PM25 modeli eğitiliyor...\n",
      "-> PM25 için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: RANDOMFOREST - SO2 modeli eğitiliyor...\n",
      "-> SO2 için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: MLP - NO2 modeli eğitiliyor...\n",
      "-> NO2 için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: MLP - O3 modeli eğitiliyor...\n",
      "-> O3 için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: MLP - PM10 modeli eğitiliyor...\n",
      "-> PM10 için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: MLP - PM25 modeli eğitiliyor...\n",
      "-> PM25 için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: MLP - SO2 modeli eğitiliyor...\n",
      "-> SO2 için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: KNN - NO2 modeli eğitiliyor...\n",
      "-> NO2 için KNN modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: KNN - O3 modeli eğitiliyor...\n",
      "-> O3 için KNN modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: KNN - PM10 modeli eğitiliyor...\n",
      "-> PM10 için KNN modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: KNN - PM25 modeli eğitiliyor...\n",
      "-> PM25 için KNN modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> FI: KNN - SO2 modeli eğitiliyor...\n",
      "-> SO2 için KNN modeli eğitildi ve kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "# BLOK4: TRAİNİNG\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import joblib\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "COUNTRY_CODE = 'tr' \n",
    "COUNTRY_CODE='tr'\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "ENGINEERED_FILE_PATH = f'{COUNTRY_CODE}_air_quality_engineered.csv'\n",
    "TARGET_POLLUTANTS = ['co', 'no2', 'o3', 'pm10', 'pm25', 'so2']\n",
    "FORECAST_HORIZON = 1\n",
    "\n",
    "# Veriyi Yükle\n",
    "df_engineered = pd.read_csv(ENGINEERED_FILE_PATH, index_col='from_date', parse_dates=True)\n",
    "print(f\"'{ENGINEERED_FILE_PATH}' yüklendi. Boyut: {df_engineered.shape}\")\n",
    "\n",
    "# Model Tanımları\n",
    "models = {\n",
    "    \"xgboost\": {\"estimator\": xgb.XGBRegressor(random_state=42), \"params\": {'n_estimators': [100, 300], 'max_depth': [5, 7]}},\n",
    "    \"randomforest\": {\"estimator\": RandomForestRegressor(random_state=42), \"params\": {'n_estimators': [100, 200], 'max_depth': [10, 20]}},\n",
    "    \"mlp\": {\"estimator\": MLPRegressor(random_state=42, max_iter=500, early_stopping=True), \"params\": {'hidden_layer_sizes': [(64, 32)], 'alpha': [0.001, 0.05]}},\n",
    "    \"knn\": {\"estimator\": KNeighborsRegressor(), \"params\": {'n_neighbors': [7, 15], 'weights': ['uniform', 'distance']}}\n",
    "}\n",
    "\n",
    "available_targets = [p for p in TARGET_POLLUTANTS if p in df_engineered.columns]\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    for target in available_targets:\n",
    "        print(f\"\\n>>> {COUNTRY_CODE.upper()}: {model_name.upper()} - {target.upper()} modeli eğitiliyor...\")\n",
    "        \n",
    "        # 1. Veri Hazırlama\n",
    "        features_to_use = [col for col in df_engineered.columns if col not in TARGET_POLLUTANTS + ['country']]\n",
    "        X = df_engineered[features_to_use].copy()\n",
    "        y = df_engineered[[target]].shift(-FORECAST_HORIZON)\n",
    "        X, y = X.iloc[:-FORECAST_HORIZON], y.iloc[:-FORECAST_HORIZON]\n",
    "        X.dropna(axis=1, how='all', inplace=True); X.fillna(0, inplace=True)\n",
    "        \n",
    "        X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        # 2. Test için nesneleri oluştur ve kaydet\n",
    "        x_scaler = RobustScaler().fit(X_train_full)\n",
    "        y_scaler = RobustScaler().fit(y_train_full)\n",
    "        k_val = min(20 if model_name == 'knn' else 35, X_train_full.shape[1])\n",
    "        X_train_s = pd.DataFrame(x_scaler.transform(X_train_full), columns=X_train_full.columns)\n",
    "        selector = SelectKBest(score_func=f_regression, k=k_val).fit(X_train_s, y_train_full.values.ravel())\n",
    "        \n",
    "        joblib.dump(x_scaler, f'x_scaler_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        joblib.dump(y_scaler, f'y_scaler_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        joblib.dump(selector, f'selector_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        X_test.to_csv(f'X_test_{target}_{COUNTRY_CODE}.csv')\n",
    "        y_test.to_csv(f'y_test_{target}_{COUNTRY_CODE}.csv')\n",
    "\n",
    "        # 3. Hiperparametre Optimizasyonu\n",
    "        X_train_hp_f = selector.transform(X_train_s)\n",
    "        y_train_hp_s = y_scaler.transform(y_train_full)\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model_info[\"estimator\"], param_distributions=model_info[\"params\"],\n",
    "            n_iter=4, cv=tscv, scoring='neg_root_mean_squared_error', random_state=42, n_jobs=-1\n",
    "        )\n",
    "        random_search.fit(X_train_hp_f, y_train_hp_s.ravel())\n",
    "        best_params = random_search.best_params_\n",
    "        \n",
    "        # 4. Final Modeli Tüm Eğitim Verisiyle Eğit ve Kaydet\n",
    "        final_model = model_info[\"estimator\"].set_params(**best_params)\n",
    "        final_model.fit(X_train_hp_f, y_train_hp_s.ravel())\n",
    "        joblib.dump(final_model, f'model_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        \n",
    "        print(f\"-> {target.upper()} için {model_name.upper()} modeli eğitildi ve kaydedildi.\")import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import joblib\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- AYARLAR (Bu satırı her ülke notebook'unda güncelleyin) ---\n",
    "COUNTRY_CODE = 'tr' \n",
    "COUNTRY_CODE='tr'\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "ENGINEERED_FILE_PATH = f'{COUNTRY_CODE}_air_quality_engineered.csv'\n",
    "TARGET_POLLUTANTS = ['co', 'no2', 'o3', 'pm10', 'pm25', 'so2']\n",
    "FORECAST_HORIZON = 1\n",
    "\n",
    "# Veriyi Yükle\n",
    "df_engineered = pd.read_csv(ENGINEERED_FILE_PATH, index_col='from_date', parse_dates=True)\n",
    "print(f\"'{ENGINEERED_FILE_PATH}' yüklendi. Boyut: {df_engineered.shape}\")\n",
    "\n",
    "# Model Tanımları\n",
    "models = {\n",
    "    \"xgboost\": {\"estimator\": xgb.XGBRegressor(random_state=42), \"params\": {'n_estimators': [100, 300], 'max_depth': [5, 7]}},\n",
    "    \"randomforest\": {\"estimator\": RandomForestRegressor(random_state=42), \"params\": {'n_estimators': [100, 200], 'max_depth': [10, 20]}},\n",
    "    \"mlp\": {\"estimator\": MLPRegressor(random_state=42, max_iter=500, early_stopping=True), \"params\": {'hidden_layer_sizes': [(64, 32)], 'alpha': [0.001, 0.05]}},\n",
    "    \"knn\": {\"estimator\": KNeighborsRegressor(), \"params\": {'n_neighbors': [7, 15], 'weights': ['uniform', 'distance']}}\n",
    "}\n",
    "\n",
    "available_targets = [p for p in TARGET_POLLUTANTS if p in df_engineered.columns]\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    for target in available_targets:\n",
    "        print(f\"\\n>>> {COUNTRY_CODE.upper()}: {model_name.upper()} - {target.upper()} modeli eğitiliyor...\")\n",
    "        \n",
    "        # 1. Veri Hazırlama\n",
    "        features_to_use = [col for col in df_engineered.columns if col not in TARGET_POLLUTANTS + ['country']]\n",
    "        X = df_engineered[features_to_use].copy()\n",
    "        y = df_engineered[[target]].shift(-FORECAST_HORIZON)\n",
    "        X, y = X.iloc[:-FORECAST_HORIZON], y.iloc[:-FORECAST_HORIZON]\n",
    "        X.dropna(axis=1, how='all', inplace=True); X.fillna(0, inplace=True)\n",
    "        \n",
    "        X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        # 2. Test için nesneleri oluştur ve kaydet\n",
    "        x_scaler = RobustScaler().fit(X_train_full)\n",
    "        y_scaler = RobustScaler().fit(y_train_full)\n",
    "        k_val = min(20 if model_name == 'knn' else 35, X_train_full.shape[1])\n",
    "        X_train_s = pd.DataFrame(x_scaler.transform(X_train_full), columns=X_train_full.columns)\n",
    "        selector = SelectKBest(score_func=f_regression, k=k_val).fit(X_train_s, y_train_full.values.ravel())\n",
    "        \n",
    "        joblib.dump(x_scaler, f'x_scaler_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        joblib.dump(y_scaler, f'y_scaler_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        joblib.dump(selector, f'selector_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        X_test.to_csv(f'X_test_{target}_{COUNTRY_CODE}.csv')\n",
    "        y_test.to_csv(f'y_test_{target}_{COUNTRY_CODE}.csv')\n",
    "\n",
    "        # 3. Hiperparametre Optimizasyonu\n",
    "        X_train_hp_f = selector.transform(X_train_s)\n",
    "        y_train_hp_s = y_scaler.transform(y_train_full)\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model_info[\"estimator\"], param_distributions=model_info[\"params\"],\n",
    "            n_iter=4, cv=tscv, scoring='neg_root_mean_squared_error', random_state=42, n_jobs=-1\n",
    "        )\n",
    "        random_search.fit(X_train_hp_f, y_train_hp_s.ravel())\n",
    "        best_params = random_search.best_params_\n",
    "        \n",
    "        # 4. Final Modeli Tüm Eğitim Verisiyle Eğit ve Kaydet\n",
    "        final_model = model_info[\"estimator\"].set_params(**best_params)\n",
    "        final_model.fit(X_train_hp_f, y_train_hp_s.ravel())\n",
    "        joblib.dump(final_model, f'model_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        \n",
    "        print(f\"-> {target.upper()} için {model_name.upper()} modeli eğitildi ve kaydedildi.\")import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import joblib\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "COUNTRY_CODE = 'fi' \n",
    "ENGINEERED_FILE_PATH = f'{COUNTRY_CODE}_air_quality_engineered.csv'\n",
    "TARGET_POLLUTANTS = ['co', 'no2', 'o3', 'pm10', 'pm25', 'so2']\n",
    "FORECAST_HORIZON = 1\n",
    "\n",
    "# Veriyi yükleme\n",
    "df_engineered = pd.read_csv(ENGINEERED_FILE_PATH, index_col='from_date', parse_dates=True)\n",
    "print(f\"'{ENGINEERED_FILE_PATH}' yüklendi. Boyut: {df_engineered.shape}\")\n",
    "\n",
    "# Model tanımları\n",
    "models = {\n",
    "    \"xgboost\": {\"estimator\": xgb.XGBRegressor(random_state=42), \"params\": {'n_estimators': [100, 300], 'max_depth': [5, 7]}},\n",
    "    \"randomforest\": {\"estimator\": RandomForestRegressor(random_state=42), \"params\": {'n_estimators': [100, 200], 'max_depth': [10, 20]}},\n",
    "    \"mlp\": {\"estimator\": MLPRegressor(random_state=42, max_iter=500, early_stopping=True), \"params\": {'hidden_layer_sizes': [(64, 32)], 'alpha': [0.001, 0.05]}},\n",
    "    \"knn\": {\"estimator\": KNeighborsRegressor(), \"params\": {'n_neighbors': [7, 15], 'weights': ['uniform', 'distance']}}\n",
    "}\n",
    "\n",
    "available_targets = [p for p in TARGET_POLLUTANTS if p in df_engineered.columns]\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    for target in available_targets:\n",
    "        print(f\"\\n>>> {COUNTRY_CODE.upper()}: {model_name.upper()} - {target.upper()} modeli eğitiliyor...\")\n",
    "        \n",
    "        # Veri hazırlama\n",
    "        features_to_use = [col for col in df_engineered.columns if col not in TARGET_POLLUTANTS + ['country']]\n",
    "        X = df_engineered[features_to_use].copy()\n",
    "        y = df_engineered[[target]].shift(-FORECAST_HORIZON)\n",
    "        X, y = X.iloc[:-FORECAST_HORIZON], y.iloc[:-FORECAST_HORIZON]\n",
    "        X.dropna(axis=1, how='all', inplace=True); X.fillna(0, inplace=True)\n",
    "        \n",
    "        X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        # Test için nesneleri oluşturma ve kaydetme\n",
    "        x_scaler = RobustScaler().fit(X_train_full)\n",
    "        y_scaler = RobustScaler().fit(y_train_full)\n",
    "        k_val = min(20 if model_name == 'knn' else 35, X_train_full.shape[1])\n",
    "        X_train_s = pd.DataFrame(x_scaler.transform(X_train_full), columns=X_train_full.columns)\n",
    "        selector = SelectKBest(score_func=f_regression, k=k_val).fit(X_train_s, y_train_full.values.ravel())\n",
    "        \n",
    "        joblib.dump(x_scaler, f'x_scaler_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        joblib.dump(y_scaler, f'y_scaler_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        joblib.dump(selector, f'selector_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        X_test.to_csv(f'X_test_{target}_{COUNTRY_CODE}.csv')\n",
    "        y_test.to_csv(f'y_test_{target}_{COUNTRY_CODE}.csv')\n",
    "\n",
    "        # Hiperparametre optimizasyonu\n",
    "        X_train_hp_f = selector.transform(X_train_s)\n",
    "        y_train_hp_s = y_scaler.transform(y_train_full)\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model_info[\"estimator\"], param_distributions=model_info[\"params\"],\n",
    "            n_iter=4, cv=tscv, scoring='neg_root_mean_squared_error', random_state=42, n_jobs=-1\n",
    "        )\n",
    "        random_search.fit(X_train_hp_f, y_train_hp_s.ravel())\n",
    "        best_params = random_search.best_params_\n",
    "        \n",
    "        # Final modeli tüm eğitim verisiyle eğitme ve kaydetme\n",
    "        final_model = model_info[\"estimator\"].set_params(**best_params)\n",
    "        final_model.fit(X_train_hp_f, y_train_hp_s.ravel())\n",
    "        joblib.dump(final_model, f'model_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        \n",
    "        print(f\"-> {target.upper()} için {model_name.upper()} modeli eğitildi ve kaydedildi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
