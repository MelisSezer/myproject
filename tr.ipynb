{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8480054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri çekme aralığı: 2018-07-13 - 2025-07-11\n",
      "TR bbox 26.0433512713,35.8215347357,44.7939896991,42.1414848903 içindeki sensörler çekiliyor...\n",
      "Sayfa 1 çekildi, 484 konum eklendi.\n",
      "\n",
      "Toplam 1862 sensör için günlük veri çekiliyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sensör Verileri Çekiliyor:   1%|          | 12/1862 [00:17<1:09:02,  2.24s/it]HTTP 429 - {\"detail\":\"Too many requests\"}\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensör 16526 için hata (Sayfa 2): {\"detail\":\"Too many requests\"}. Bekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP 429 - {\"detail\":\"Too many requests\"}\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensör 16526 için hata (Sayfa 2): {\"detail\":\"Too many requests\"}. Bekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP 429 - {\"detail\":\"Too many requests\"}\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensör 16526 için hata (Sayfa 2): {\"detail\":\"Too many requests\"}. Bekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP 429 - {\"detail\":\"Too many requests\"}\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensör 16526 için hata (Sayfa 2): {\"detail\":\"Too many requests\"}. Bekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP 429 - {\"detail\":\"Too many requests\"}\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensör 16526 için hata (Sayfa 2): {\"detail\":\"Too many requests\"}. Bekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sensör Verileri Çekiliyor:   1%|          | 20/1862 [01:26<1:29:52,  2.93s/it]HTTP 429 - {\"detail\":\"Too many requests\"}\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensör 16643 için hata (Sayfa 3): {\"detail\":\"Too many requests\"}. Bekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP 429 - {\"detail\":\"Too many requests\"}\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensör 16643 için hata (Sayfa 3): {\"detail\":\"Too many requests\"}. Bekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP 429 - {\"detail\":\"Too many requests\"}\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensör 16643 için hata (Sayfa 3): {\"detail\":\"Too many requests\"}. Bekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sensör Verileri Çekiliyor:   2%|▏         | 31/1862 [02:25<58:32,  1.92s/it]  HTTP 429 - {\"detail\":\"Too many requests\"}\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sensör 5079342 için hata (Sayfa 2): {\"detail\":\"Too many requests\"}. Bekleniyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sensör Verileri Çekiliyor:   2%|▏         | 31/1862 [02:36<2:34:32,  5.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPRateLimitError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mget_daily_by_sid\u001b[39m\u001b[34m(sids, start, end, sensor_map, sleep_time)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     resp = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasurements\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43msensors_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdays\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime_from\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resp.results: \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\melis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openaq\\_sync\\models\\measurements.py:72\u001b[39m, in \u001b[36mMeasurements.list\u001b[39m\u001b[34m(self, sensors_id, data, rollup, datetime_from, datetime_to, page, limit)\u001b[39m\n\u001b[32m     70\u001b[39m path = build_measurements_path(sensors_id, data, rollup)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m measurements_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MeasurementsResponse.read_response(measurements_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\melis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openaq\\_sync\\client.py:106\u001b[39m, in \u001b[36mOpenAQ._get\u001b[39m\u001b[34m(self, path, params, headers)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_get\u001b[39m(\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    101\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m     headers: Mapping[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    105\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\melis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openaq\\_sync\\client.py:93\u001b[39m, in \u001b[36mOpenAQ._do\u001b[39m\u001b[34m(self, method, path, params, headers)\u001b[39m\n\u001b[32m     92\u001b[39m url = \u001b[38;5;28mself\u001b[39m._base_url + path\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_headers\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mself\u001b[39m._set_rate_limit(data.headers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\melis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openaq\\_sync\\transport.py:29\u001b[39m, in \u001b[36mTransport.send_request\u001b[39m\u001b[34m(self, method, url, params, headers)\u001b[39m\n\u001b[32m     28\u001b[39m res = \u001b[38;5;28mself\u001b[39m.client.send(request)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\melis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openaq\\shared\\transport.py:93\u001b[39m, in \u001b[36mcheck_response\u001b[39m\u001b[34m(res)\u001b[39m\n\u001b[32m     92\u001b[39m     logger.exception(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHTTP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPRateLimitError(res.text)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m res.status_code == HTTPStatus.UNAUTHORIZED:\n",
      "\u001b[31mHTTPRateLimitError\u001b[39m: {\"detail\":\"Too many requests\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mget_daily_by_sid\u001b[39m\u001b[34m(sids, start, end, sensor_map, sleep_time)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     wait_time = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresets in \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m.split(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]) + \u001b[32m1\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m saniye bekleniyor...\u001b[39m\u001b[33m\"\u001b[39m); time.sleep(wait_time)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     71\u001b[39m sensor_ids, sensor_country_map = get_sensor_ids(BBOX_TR, client, COUNTRY_CODE)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sensor_ids:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     raw_df = \u001b[43mget_daily_by_sid\u001b[49m\u001b[43m(\u001b[49m\u001b[43msensor_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaslangic_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbitis_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor_country_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_df.empty:\n\u001b[32m     75\u001b[39m         raw_df.to_csv(RAW_DATA_FILE, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mget_daily_by_sid\u001b[39m\u001b[34m(sids, start, end, sensor_map, sleep_time)\u001b[39m\n\u001b[32m     59\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m saniye bekleniyor...\u001b[39m\u001b[33m\"\u001b[39m); time.sleep(wait_time)\n\u001b[32m     60\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m                 \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(all_data)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# BLOK1:APİDEN VERİ ÇEKME\n",
    "import pandas as pd\n",
    "from openaq import OpenAQ\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "API_KEY = \"beb87bdeb247d901be44e59bd044aa34556b5c4752592b5a2cd9da243a25a466\"\n",
    "BBOX_TR = \"26.0433512713,35.8215347357,44.7939896991,42.1414848903\" # Türkiye koordinatları\n",
    "COUNTRY_CODE = 'TR'\n",
    "RAW_DATA_FILE = 'openaq_raw_data_tr.csv' \n",
    "\n",
    "client = OpenAQ(api_key=API_KEY)\n",
    "# Verilen koordinattaki sensörler çekiliyor\n",
    "def get_sensor_ids(bbox, client, country_code):\n",
    "    all_sensor_ids, sensor_country_map = [], {}\n",
    "    page = 1\n",
    "    print(f\"{country_code} bbox {bbox} içindeki sensörler çekiliyor...\")\n",
    "    while True:\n",
    "        try:\n",
    "            resp = client.locations.list(bbox=bbox, limit=1000, page=page)\n",
    "            if not resp.results: break\n",
    "            for loc in resp.results:\n",
    "                if hasattr(loc, 'sensors') and loc.sensors:\n",
    "                    for sensor in loc.sensors:\n",
    "                        sensor_id = getattr(sensor, 'id', None)\n",
    "                        if sensor_id:\n",
    "                            all_sensor_ids.append(sensor_id)\n",
    "                            sensor_country_map[sensor_id] = getattr(loc.country, 'code', country_code)\n",
    "            print(f\"Sayfa {page} çekildi, {len(resp.results)} konum eklendi.\")\n",
    "            page += 1; time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print(f\"Hata (Sayfa {page}): {e}. 5 saniye bekleniyor.\"); time.sleep(5)\n",
    "            continue\n",
    "    return list(set(all_sensor_ids)), sensor_country_map\n",
    "# Sensörlerden günlük verilerl çekiliyor\n",
    "def get_daily_by_sid(sids, start, end, sensor_map, sleep_time=0.3):\n",
    "    all_data = []\n",
    "    print(f\"\\nToplam {len(sids)} sensör için günlük veri çekiliyor...\")\n",
    "    for sid in tqdm(sids, desc=\"Sensör Verileri Çekiliyor\"):\n",
    "        page = 1\n",
    "        while True:\n",
    "            try:\n",
    "                resp = client.measurements.list(sensors_id=sid, data=\"days\", datetime_from=start, datetime_to=end, limit=1000, page=page)\n",
    "                if not resp.results: break\n",
    "                for res in resp.results:\n",
    "                    all_data.append({\n",
    "                        \"from_date\": res.period.datetime_from.utc,\n",
    "                        \"name\": res.parameter.name,\n",
    "                        \"value\": res.value,\n",
    "                        \"unit\": res.parameter.units,\n",
    "                        \"country\": sensor_map.get(sid, 'Unknown')\n",
    "                    })\n",
    "                page += 1; time.sleep(sleep_time)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nSensör {sid} için hata (Sayfa {page}): {e}. Bekleniyor...\")\n",
    "                try:\n",
    "                    wait_time = int(str(e).split('resets in ')[1].split(' ')[0]) + 1\n",
    "                    print(f\"{wait_time} saniye bekleniyor...\"); time.sleep(wait_time)\n",
    "                except:\n",
    "                    time.sleep(10)\n",
    "                continue\n",
    "    return pd.DataFrame(all_data)\n",
    "#7 yıllık veri alınıyor sadece\n",
    "bitis = datetime.now()\n",
    "baslangic = bitis - timedelta(days=365 * 7)\n",
    "baslangic_str = baslangic.strftime(\"%Y-%m-%d\")\n",
    "bitis_str = bitis.strftime(\"%Y-%m-%d\")\n",
    "print(f\"Veri çekme aralığı: {baslangic_str} - {bitis_str}\")\n",
    "\n",
    "sensor_ids, sensor_country_map = get_sensor_ids(BBOX_TR, client, COUNTRY_CODE)\n",
    "if sensor_ids:\n",
    "    raw_df = get_daily_by_sid(sensor_ids, baslangic_str, bitis_str, sensor_country_map)\n",
    "    if not raw_df.empty:\n",
    "        raw_df.to_csv(RAW_DATA_FILE, index=False)\n",
    "        print(f\"\\nİşlenmemiş ham veri başarıyla '{RAW_DATA_FILE}' dosyasına kaydedildi\")\n",
    "        print(f\"Toplam {len(raw_df)} satır ham veri çekildi\")\n",
    "    else:\n",
    "        print(\"API'den veri çekilemedi\")\n",
    "else:\n",
    "    print(\"Belirtilen alanda sensör bulunamadı\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a59f506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'openaq_raw_data_tr.csv' dosyasından ham veri okunuyor...\n",
      "\n",
      "Birim dönüşümü başlıyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Birimler Dönüştürülüyor: 100%|██████████| 663555/663555 [00:18<00:00, 36699.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Veri işleme ve pivotlama başlıyor...\n",
      "\n",
      "İşlenmiş (logaritmasız) veri 'tr_air_quality_processed.csv' dosyasına kaydedildi.\n",
      "İşlenmiş verinin ilk 5 satırı:\n",
      "                  from_date           co        no2         o3  pm1  \\\n",
      "0 2016-12-11 21:00:00+00:00  4321.000000  53.000000  53.500000  NaN   \n",
      "1 2016-12-13 21:00:00+00:00          NaN   6.500000  73.000000  NaN   \n",
      "2 2016-12-16 21:00:00+00:00   280.000000   0.000000  37.000000  NaN   \n",
      "3 2016-12-21 21:00:00+00:00    84.000000   3.000000  90.000000  NaN   \n",
      "4 2016-12-22 21:00:00+00:00  1526.944444  59.106897  26.691837  NaN   \n",
      "\n",
      "         pm10  pm25  pressure  relativehumidity         so2  temperature  \\\n",
      "0  304.500000   NaN       NaN               NaN  198.500000          NaN   \n",
      "1         NaN   NaN       NaN               NaN    5.000000          NaN   \n",
      "2   10.000000   NaN       NaN               NaN   16.000000          NaN   \n",
      "3   15.500000   NaN       NaN               NaN    8.500000          NaN   \n",
      "4   70.526804   NaN       NaN               NaN   32.945223          NaN   \n",
      "\n",
      "   um003  \n",
      "0    NaN  \n",
      "1    NaN  \n",
      "2    NaN  \n",
      "3    NaN  \n",
      "4    NaN  \n"
     ]
    }
   ],
   "source": [
    "# BLOK2: VERİ ÖN İŞLEME \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DATA_FILE = 'openaq_raw_data_tr.csv'\n",
    "PROCESSED_FILE_PATH = 'tr_air_quality_processed.csv'\n",
    "COUNTRY_CODE = 'TR'\n",
    "# Kirleticilerin birimleri eşitleniyor\n",
    "def process_data(file_path, country_code):\n",
    "    print(f\"'{file_path}' dosyasından ham veri okunuyor...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(\"\\nBirim dönüşümü başlıyor...\")\n",
    "    MOLAR_MASSES = {'co': 28.01, 'no2': 46.01, 'o3': 48.00, 'so2': 64.07} # internetten alınan değerler\n",
    "    MOLAR_VOLUME = 24.45\n",
    "    converted_values = []\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Birimler Dönüştürülüyor\"):\n",
    "        param, unit, value = str(row['name']).lower(), str(row['unit']).lower(), row['value']\n",
    "        new_value = None\n",
    "        if pd.notna(value) and value >= 0:\n",
    "            if unit == 'µg/m³': new_value = value\n",
    "            else:\n",
    "                molar_mass = MOLAR_MASSES.get(param)\n",
    "                if molar_mass is not None:\n",
    "                    if unit == 'ppm': new_value = (value * molar_mass * 1000) / MOLAR_VOLUME\n",
    "                    elif unit == 'ppb': new_value = (value * molar_mass) / MOLAR_VOLUME\n",
    "                    else: new_value = value\n",
    "                else: new_value = value\n",
    "        converted_values.append(new_value)\n",
    "    df['value_converted'] = converted_values\n",
    "    \n",
    "    # Pivotlama\n",
    "    print(\"\\nVeri işleme ve pivotlama başlıyor...\")\n",
    "    df['from_date'] = pd.to_datetime(df['from_date'], errors='coerce')\n",
    "    df.dropna(subset=['from_date', 'value_converted'], inplace=True)\n",
    "    df_filtered = df[df['country'] == country_code].copy()\n",
    "    \n",
    "    df_pivot = df_filtered.pivot_table(index='from_date', columns='name', values='value_converted', aggfunc='mean').reset_index()\n",
    "    df_pivot.columns.name = None\n",
    "    df_pivot.sort_values(by='from_date', ascending=True, inplace=True)\n",
    "    \n",
    "    return df_pivot\n",
    "\n",
    "try:\n",
    "    processed_df = process_data(RAW_DATA_FILE, COUNTRY_CODE)\n",
    "    processed_df.to_csv(PROCESSED_FILE_PATH, index=False)\n",
    "    print(f\"\\nİşlenmiş (logaritmasız) veri '{PROCESSED_FILE_PATH}' dosyasına kaydedildi.\")\n",
    "    print(\"İşlenmiş verinin ilk 5 satırı:\")\n",
    "    print(processed_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Hata: '{RAW_DATA_FILE}' bulunamadı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4b01d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tr_air_quality_processed.csv' dosyasından veri okunuyor...\n",
      "Veri başarıyla yüklendi. Boyut: (1624, 11)\n",
      "\n",
      "Eksik veri yönetimi başlıyor...\n",
      "Eksik veri yönetimi tamamlandı\n",
      "\n",
      "Aşırı aykırı değerler %99.9 persentil ile kırpılıyor...\n",
      " 'co' sütunu, üst sınır '3719868.69' ile kırpıldı.\n",
      " 'no2' sütunu, üst sınır '187.53' ile kırpıldı.\n",
      " 'o3' sütunu, üst sınır '598.78' ile kırpıldı.\n",
      " 'pm1' sütunu, üst sınır '48.47' ile kırpıldı.\n",
      " 'pm10' sütunu, üst sınır '410914.25' ile kırpıldı.\n",
      " 'pm25' sütunu, üst sınır '14705.52' ile kırpıldı.\n",
      " 'pressure' sütunu, üst sınır '1032.75' ile kırpıldı.\n",
      " 'relativehumidity' sütunu, üst sınır '81.70' ile kırpıldı.\n",
      " 'so2' sütunu, üst sınır '238.17' ile kırpıldı.\n",
      " 'temperature' sütunu, üst sınır '30.61' ile kırpıldı.\n",
      " 'um003' sütunu, üst sınır '13209.32' ile kırpıldı.\n",
      "\n",
      "Logaritmik dönüşüm uygulanıyor...\n",
      "\n",
      "Öznitelik mühendisliği başlıyor...\n",
      "Öznitelik mühendisliği tamamlandı\n",
      "\n",
      "Temizlenmiş veri 'tr_air_quality_engineered.csv' dosyasına kaydedildi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melis\\AppData\\Local\\Temp\\ipykernel_32292\\1094720621.py:26: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True); df.fillna(method='bfill', inplace=True)\n",
      "C:\\Users\\melis\\AppData\\Local\\Temp\\ipykernel_32292\\1094720621.py:48: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# BLOK3: FEATURE ENGINEERING\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PROCESSED_FILE_PATH = 'tr_air_quality_processed.csv'\n",
    "ENGINEERED_FILE_PATH = 'tr_air_quality_engineered.csv'\n",
    "\n",
    "print(f\"'{PROCESSED_FILE_PATH}' dosyasından veri okunuyor...\")\n",
    "try:\n",
    "    df = pd.read_csv(PROCESSED_FILE_PATH, parse_dates=['from_date'])\n",
    "    df.set_index('from_date', inplace=True)\n",
    "    print(f\"Veri başarıyla yüklendi. Boyut: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Hata: '{PROCESSED_FILE_PATH}' bulunamadı\")\n",
    "    exit()\n",
    "\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Eksik veri interpolate yöntemi ile dolduruldu\n",
    "print(\"\\nEksik veri yönetimi başlıyor...\")\n",
    "pollutant_cols = [col for col in df.columns if df[col].dtype in ['float64', 'int64']]\n",
    "# Ülke sütunu çıkarıldı\n",
    "if 'country' in pollutant_cols: pollutant_cols.remove('country')\n",
    "\n",
    "df[pollutant_cols] = df[pollutant_cols].interpolate(method='time', limit_direction='both')\n",
    "df.fillna(method='ffill', inplace=True); df.fillna(method='bfill', inplace=True)\n",
    "print(\"Eksik veri yönetimi tamamlandı\")\n",
    "\n",
    "\n",
    "# Aşırı değerler kırpıldı\n",
    "print(\"\\nAşırı aykırı değerler %99.9 persentil ile kırpılıyor...\")\n",
    "for col in pollutant_cols:\n",
    "    ust_limit = df[col].quantile(0.999)\n",
    "    df[col] = df[col].clip(upper=ust_limit)\n",
    "    print(f\" '{col}' sütunu, üst sınır '{ust_limit:.2f}' ile kırpıldı.\")\n",
    "\n",
    "# Logaritmik dönüşüm yapıldı her bir col için hatayı minizimize etmek lazım çünkü\n",
    "print(\"\\nLogaritmik dönüşüm uygulanıyor...\")\n",
    "for col in pollutant_cols:\n",
    "    df[col] = np.log1p(df[col])\n",
    "\n",
    "# Türetilmiş bazı yararlı öznitelikler ekleniyor\n",
    "print(\"\\nÖznitelik mühendisliği başlıyor...\")\n",
    "df['year'] = df.index.year; df['month'] = df.index.month; df['day_of_week'] = df.index.dayofweek\n",
    "for pollutant in pollutant_cols:\n",
    "    for lag in [1, 3, 7]: df[f'{pollutant}_lag_{lag}d'] = df[pollutant].shift(lag)\n",
    "    for window in [3, 7]: df[f'{pollutant}_roll_mean_{window}d'] = df[pollutant].rolling(window, closed='left').mean()\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "print(\"Öznitelik mühendisliği tamamlandı\")\n",
    "\n",
    "df.to_csv(ENGINEERED_FILE_PATH, index=True)\n",
    "print(f\"\\nTemizlenmiş veri '{ENGINEERED_FILE_PATH}' dosyasına kaydedildi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a382cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tr_air_quality_engineered.csv' yüklendi. Boyut: (1624, 69)\n",
      "\n",
      ">>> TR: XGBOOST - CO modeli eğitiliyor...\n",
      "-> CO için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: XGBOOST - NO2 modeli eğitiliyor...\n",
      "-> NO2 için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: XGBOOST - O3 modeli eğitiliyor...\n",
      "-> O3 için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: XGBOOST - PM10 modeli eğitiliyor...\n",
      "-> PM10 için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: XGBOOST - PM25 modeli eğitiliyor...\n",
      "-> PM25 için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: XGBOOST - SO2 modeli eğitiliyor...\n",
      "-> SO2 için XGBOOST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: RANDOMFOREST - CO modeli eğitiliyor...\n",
      "-> CO için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: RANDOMFOREST - NO2 modeli eğitiliyor...\n",
      "-> NO2 için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: RANDOMFOREST - O3 modeli eğitiliyor...\n",
      "-> O3 için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: RANDOMFOREST - PM10 modeli eğitiliyor...\n",
      "-> PM10 için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: RANDOMFOREST - PM25 modeli eğitiliyor...\n",
      "-> PM25 için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: RANDOMFOREST - SO2 modeli eğitiliyor...\n",
      "-> SO2 için RANDOMFOREST modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: MLP - CO modeli eğitiliyor...\n",
      "-> CO için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: MLP - NO2 modeli eğitiliyor...\n",
      "-> NO2 için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: MLP - O3 modeli eğitiliyor...\n",
      "-> O3 için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: MLP - PM10 modeli eğitiliyor...\n",
      "-> PM10 için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: MLP - PM25 modeli eğitiliyor...\n",
      "-> PM25 için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: MLP - SO2 modeli eğitiliyor...\n",
      "-> SO2 için MLP modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: KNN - CO modeli eğitiliyor...\n",
      "-> CO için KNN modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: KNN - NO2 modeli eğitiliyor...\n",
      "-> NO2 için KNN modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: KNN - O3 modeli eğitiliyor...\n",
      "-> O3 için KNN modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: KNN - PM10 modeli eğitiliyor...\n",
      "-> PM10 için KNN modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: KNN - PM25 modeli eğitiliyor...\n",
      "-> PM25 için KNN modeli eğitildi ve kaydedildi.\n",
      "\n",
      ">>> TR: KNN - SO2 modeli eğitiliyor...\n",
      "-> SO2 için KNN modeli eğitildi ve kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "# BLOK4: TRAİNİNG\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import joblib\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "COUNTRY_CODE = 'tr' \n",
    "ENGINEERED_FILE_PATH = f'{COUNTRY_CODE}_air_quality_engineered.csv'\n",
    "TARGET_POLLUTANTS = ['co', 'no2', 'o3', 'pm10', 'pm25', 'so2']\n",
    "FORECAST_HORIZON = 1\n",
    "\n",
    "# Veri yükleme\n",
    "df_engineered = pd.read_csv(ENGINEERED_FILE_PATH, index_col='from_date', parse_dates=True)\n",
    "print(f\"'{ENGINEERED_FILE_PATH}' yüklendi. Boyut: {df_engineered.shape}\")\n",
    "\n",
    "# Model tanımları\n",
    "models = {\n",
    "    \"xgboost\": {\"estimator\": xgb.XGBRegressor(random_state=42), \"params\": {'n_estimators': [100, 300], 'max_depth': [5, 7]}},\n",
    "    \"randomforest\": {\"estimator\": RandomForestRegressor(random_state=42), \"params\": {'n_estimators': [100, 200], 'max_depth': [10, 20]}},\n",
    "    \"mlp\": {\"estimator\": MLPRegressor(random_state=42, max_iter=500, early_stopping=True), \"params\": {'hidden_layer_sizes': [(64, 32)], 'alpha': [0.001, 0.05]}},\n",
    "    \"knn\": {\"estimator\": KNeighborsRegressor(), \"params\": {'n_neighbors': [7, 15], 'weights': ['uniform', 'distance']}}\n",
    "}\n",
    "\n",
    "available_targets = [p for p in TARGET_POLLUTANTS if p in df_engineered.columns]\n",
    "\n",
    "for model_name, model_info in models.items():\n",
    "    for target in available_targets:\n",
    "        print(f\"\\n>>> {COUNTRY_CODE.upper()}: {model_name.upper()} - {target.upper()} modeli eğitiliyor...\")\n",
    "        \n",
    "        # 1. Veri hazırlama\n",
    "        features_to_use = [col for col in df_engineered.columns if col not in TARGET_POLLUTANTS + ['country']]\n",
    "        X = df_engineered[features_to_use].copy()\n",
    "        y = df_engineered[[target]].shift(-FORECAST_HORIZON)\n",
    "        X, y = X.iloc[:-FORECAST_HORIZON], y.iloc[:-FORECAST_HORIZON]\n",
    "        X.dropna(axis=1, how='all', inplace=True); X.fillna(0, inplace=True)\n",
    "        \n",
    "        X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        # Test için nesne oluşturma ve kaydetme\n",
    "        x_scaler = RobustScaler().fit(X_train_full)\n",
    "        y_scaler = RobustScaler().fit(y_train_full)\n",
    "        k_val = min(20 if model_name == 'knn' else 35, X_train_full.shape[1])\n",
    "        X_train_s = pd.DataFrame(x_scaler.transform(X_train_full), columns=X_train_full.columns)\n",
    "        selector = SelectKBest(score_func=f_regression, k=k_val).fit(X_train_s, y_train_full.values.ravel())\n",
    "        \n",
    "        joblib.dump(x_scaler, f'x_scaler_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        joblib.dump(y_scaler, f'y_scaler_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        joblib.dump(selector, f'selector_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        X_test.to_csv(f'X_test_{target}_{COUNTRY_CODE}.csv')\n",
    "        y_test.to_csv(f'y_test_{target}_{COUNTRY_CODE}.csv')\n",
    "\n",
    "        # Hiperparametre optimizasyonu\n",
    "        X_train_hp_f = selector.transform(X_train_s)\n",
    "        y_train_hp_s = y_scaler.transform(y_train_full)\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model_info[\"estimator\"], param_distributions=model_info[\"params\"],\n",
    "            n_iter=4, cv=tscv, scoring='neg_root_mean_squared_error', random_state=42, n_jobs=-1\n",
    "        )\n",
    "        random_search.fit(X_train_hp_f, y_train_hp_s.ravel())\n",
    "        best_params = random_search.best_params_\n",
    "        \n",
    "        # Final modelinin tüm eğitim verisiyle eğitimi ve kaydı\n",
    "        final_model = model_info[\"estimator\"].set_params(**best_params)\n",
    "        final_model.fit(X_train_hp_f, y_train_hp_s.ravel())\n",
    "        joblib.dump(final_model, f'model_{model_name}_{target}_{COUNTRY_CODE}.pkl')\n",
    "        \n",
    "        print(f\"-> {target.upper()} için {model_name.upper()} modeli eğitildi ve kaydedildi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
